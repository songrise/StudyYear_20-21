1 
a Classification rules of the DT
IF DAY_of_the_Week is Mon-Thurs AND From is NT THEN Transportation_Taken is Bus

IF DAY_of_the_Week is Mon-Thurs AND From is HK AND TO is NT THEN Transportation_Taken is Bus

IF DAY_of_the_Week is Mon-Thurs AND From is HK AND TO is HK THEN Transportation_Taken is Bus

IF DAY_of_the_Week is Mon-Thurs AND From is Kln AND TO is HK THEN Transportation_Taken is MTR

IF DAY_of_the_Week is Mon-Thurs AND From is Kln AND TO is HK THEN Transportation_Taken is Bus

IF DAY_of_the_Week IS Fri-Sun THEN Transportation_Taken is MTR



4. 
In A DT classifier (ID3), the leaf node maybe impure. In those cases, a workable strategy is majority. This may work will on some node where the information entropy is low (e.g. 1 vs 9, $I(1,9) = 0.469$). However, when information entropy is relatively high, such as 4 vs 6 $I(4,6) = 0.970$, this method may be quite ... If this is the case (information entropy of a leaf node is higher than certain threshold) knn maybe used to enhance the performance (in terms of accuracy).This will be further illustrated below.  

My proposal is to use knn to the obtain the expected value of those impure leaf with high entropy. 

Conceptually speaking, a leaf should be "near" to its sibling leaf in the sample space, which means that the majority of attributes of them should be same. Thus we can use nearby leaves to predict this leaf. To find the nearest leaves, we need to traverse the tree and find the nearby nodes. By the intrinsic of ID3/C4.5 decision tree, the split is greedy, so the node at lower depth should have higher weight on distance.

consider such a decision tree on two attributes: 
B is impure and thus is to be evaluated by knn, here we take k=5 for demonstration.
A is the nearest nodes since the parent of parent node is same (Age < 20).
D is the second nearest because it has one node same (Female)..
C is the furthest node because none of the attribute is same.

So, B node should be considered 'student' when k = 5.

In a knn perspective, an corresponding demonstration on sample space should look like this:


For univariate trees, the decision boundary should still be axis-parallel because knn is only applied to predict the value of the leaf nodes when constructing the tree. Only leaf nodes are affected (disregarding post-pruning). Intuitively, the decision boundary should be flattened compared to original tree, because leaf node are much more likely to have same value as its nearby nodes. It suggests better generalization ability.